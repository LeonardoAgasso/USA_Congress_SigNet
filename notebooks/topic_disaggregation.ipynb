{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic disaggregation\n",
    "\n",
    "### Repeat the same process on data previously disaggregated according to the categories of the vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import glob, os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "from shapely.geometry import LineString\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../local/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_members_df(members, party_codes):\n",
    "    temp_congress = members.groupby('icpsr', as_index=False)[['congress']].agg(lambda x: list(x))                                                           # group by icpsr and aggregate the congress numbers into a list\n",
    "    temp_party = members.groupby('icpsr', as_index=False)[['party_code']].agg(lambda x: list(set(x)))                                                       # group by icpsr and aggregate the party codes into a list\n",
    "    temp_congress = temp_congress.merge(temp_party)                                                                                                         # merge the two dataframes\n",
    "    temp_congress['bioname'] = temp_congress['icpsr'].map(members[['icpsr', 'bioname']].set_index('icpsr').to_dict()['bioname'])                            # insert the bioname based on the icpsr \n",
    "    temp_congress['state_abbrev'] = temp_congress['icpsr'].map(members[['icpsr', 'state_abbrev']].set_index('icpsr').to_dict()['state_abbrev'])             # insert the state_abbrev based on the icpsr\n",
    "    party_codes_dic = party_codes[['party_name', 'party_code']].set_index('party_code').to_dict()['party_name']                                             # create a dictionary for the party codes\n",
    "    temp_congress['party_name'] = temp_congress['party_code'].apply(lambda x: [party_codes_dic[y] for y in x])                                              # insert the party name based on the party code\n",
    "    return temp_congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = pd.read_csv('../dataset/HSall_members.csv')\n",
    "party_codes = pd.read_csv('../dataset/HSall_parties.csv')\n",
    "\n",
    "members_info = create_members_df(members, party_codes)\n",
    "\n",
    "member_party_dict = members_info.set_index('icpsr')['party_name'].to_dict()\t\t# member_id -> party_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edgelist_from_congress(congress, members_party_dict):\n",
    "\tedgelist = pd.DataFrame()\n",
    "\n",
    "\tfor voteid in tqdm(set(congress['id'])):                 # iterate over all votes id (ids are unique for each vote)\n",
    "\n",
    "\t\ttemp = congress[congress['id'] == voteid]            # select the rows where the vote id is equal to the current vote id            \n",
    "\n",
    "\t\tyy = temp[temp['vote']=='Yea']['icpsr']              # select the icpsr of the members that voted \"Yea\"\n",
    "\t\tnn = temp[temp['vote']=='Nay']['icpsr']                         \n",
    "\n",
    "\t\ty = itertools.combinations(yy, 2)                    # all possible combinations of 2 members that voted \"Yea\"\n",
    "\t\tn = itertools.combinations(nn, 2)                \n",
    "\t\to = itertools.product(yy, nn)                        # cartesian product of the 2 series\n",
    "\n",
    "\t\ty = pd.DataFrame(y, columns = ['source', 'target'])  # create a dataframe from the combinations of \"Yea\" voters\n",
    "\t\ty['weight'] = 1                                      # add a column with the weight of the edge\n",
    "\t\ty['count'] = 1                                         \n",
    "\n",
    "\t\tn = pd.DataFrame(n, columns = ['source', 'target'])     \n",
    "\t\tn['weight'] = 1                                         \n",
    "\t\tn['count'] = 1                                          \n",
    "\n",
    "\t\to = pd.DataFrame(o, columns = ['source', 'target'])     \n",
    "\t\to['weight'] = -1                                     # same but the link is negative                    \n",
    "\t\to['count'] = 1                                          \n",
    "\n",
    "\t\tedgelist = pd.concat([edgelist, y, n, o])\n",
    "\t\tedgelist = edgelist.groupby(['source', 'target', 'weight']).sum().reset_index()     # group by source, target and weight and sum the count                  \n",
    "\n",
    "\tedgelist = pd.concat([edgelist, pd.DataFrame({\n",
    "\t\t'source': edgelist['target'],                        # new columns based on old columns: \n",
    "\t\t'target': edgelist['source'],                        #   'newcolumn': dataframe['oldcolumn']\n",
    "\t\t'weight': edgelist['weight'],\n",
    "\t\t'count': edgelist['count']})])\n",
    "\n",
    "\tedgelist = edgelist.loc[edgelist['source'] < edgelist['target']]                    # remove duplicates\n",
    "\tedgelist = edgelist.groupby(['source', 'target', 'weight']).sum().reset_index()     # group by source, target and weight and sum the count\n",
    "\tedgelist['party'] = edgelist.apply(lambda row: 'in' if members_party_dict[row['source']] == members_party_dict[row['target']] else 'out', axis=1)   # create a column with the party of the edge\n",
    "\n",
    "\tmap_votes = edgelist.groupby(['source', 'target'])['count'].sum().to_dict()                                                                         # create a dictionary with the number of votes togheter for each pair of nodes                               \n",
    "\n",
    "\tedgelist['votes_togheter'] = edgelist[['source', 'target']].apply(lambda x: map_votes[(x['source'], x['target'])], axis=1)\n",
    "\tedgelist['perc'] = edgelist['count']/edgelist['votes_togheter']\n",
    "\n",
    "\treturn edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_path = '../dataset/download_votes_merged/*'\n",
    "congress_path_senate = '../dataset/download_votes_merged_senate/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 32/41 [00:38<00:12,  1.38s/it]/tmp/ipykernel_11888/3165551236.py:7: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "100%|██████████| 41/41 [00:53<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a weighted bipartite graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for csv_file in tqdm(glob.glob(congress_path)):\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Count the occurrences of each pair of Clausen and Peltzman categories\n",
    "    counts = df.groupby([\"Clausen\", \"Peltzman\"]).size().reset_index(name=\"count\")\n",
    "    \n",
    "    # Add edges to the graph with weights based on the counts\n",
    "    for _, row in counts.iterrows():\n",
    "        clausen_category = row[\"Clausen\"]\n",
    "        peltzman_category = row[\"Peltzman\"]\n",
    "        count = row[\"count\"]\n",
    "        \n",
    "        G.add_node(clausen_category, bipartite=0)\n",
    "        G.add_node(peltzman_category, bipartite=1)\n",
    "        G.add_edge(clausen_category, peltzman_category, weight=count)\n",
    "\n",
    "# Now you have a weighted bipartite graph (G) with edges representing the co-occurrences\n",
    "# between Clausen and Peltzman categories and weights representing the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clausen categories:  ['Agriculture', 'Civil Liberties', 'Foreign and Defense Policy', 'Government Management', 'Miscellaneous Policy', 'Social Welfare']\n",
      "Number of Clausen categories:  6\n",
      "Peltzman categories:  ['Budget Special Interest', 'Regulation General Interest', 'Regulation Special Interest', 'Domestic Social Policy', 'Defense Policy Budget', 'Defense Policy Resolutions', 'Foreign Policy Budget', 'Foreign Policy Resolutions', 'Budget General Interest', 'D. C.', 'Government Organization', 'Indian Affairs', 'Internal Organization']\n",
      "Number of Peltzman categories:  13\n"
     ]
    }
   ],
   "source": [
    "print('Clausen categories: ', [n for n in G.nodes() if G.nodes[n]['bipartite'] == 0])\n",
    "print('Number of Clausen categories: ', len([n for n in G.nodes() if G.nodes[n]['bipartite'] == 0]))\n",
    "\n",
    "print('Peltzman categories: ', [n for n in G.nodes() if G.nodes[n]['bipartite'] == 1])\n",
    "print('Number of Peltzman categories: ', len([n for n in G.nodes() if G.nodes[n]['bipartite'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kde_threshold(df, weight):\n",
    "    \n",
    "\tdef _midpoint(p1, p2):\n",
    "\t\treturn {'x': (p1['x']+p2['x'])/2, 'y': (p1['y']+p2['y'])/2}\n",
    "\n",
    "\tdef line_intersection(in_state, out_state, intersect_points):\n",
    "\t\tindex_in = np.argmax(in_state[1])\n",
    "\t\tindex_out = np.argmax(out_state[1])\n",
    "\n",
    "        # points of the mean of the distributions \n",
    "\t\tpoint_in={'x': in_state[0][index_in], 'y': in_state[1][index_in]}\n",
    "\t\tpoint_out={'x': out_state[0][index_out], 'y': out_state[1][index_out]}\n",
    "\n",
    "        # medianpoint (mean of the means) of the two distributions\n",
    "\t\tmidpoint = _midpoint(point_in, point_out)\n",
    "        \n",
    "        #find index of intersection closer to midpoint\n",
    "\t\tindex_closer = np.argmin([np.sqrt( (p[0] - midpoint['x'])**2 + (p[1] - midpoint['y'])**2 ) for p in intersect_points])\n",
    "\n",
    "        # return x value of closer intersection\n",
    "\t\treturn intersect_points[index_closer][0]\n",
    "    \n",
    "\n",
    "\t#label = \"agree\" if weight == 1 else \"disagree\"\n",
    "\tx0 = df.loc[(df['state']=='in')&(df['weight'] == weight)]['perc']\n",
    "\tx1 = df.loc[(df['state']=='out')&(df['weight'] == weight)]['perc']\n",
    "\n",
    "\tbw = len(x0)**(-1./(2+4))\n",
    "\tkde0 = gaussian_kde(x0, bw_method=bw)\n",
    "\tbw = len(x1)**(-1./(2+4))\n",
    "\tkde1 = gaussian_kde(x1, bw_method=bw)\n",
    "\n",
    "\txmin = min(x0.min(), x1.min())\n",
    "\txmax = max(x0.max(), x1.max())\n",
    "\tdx = 0.2 * (xmax - xmin) # add a 20% margin, as the kde is wider than the data\n",
    "\txmin -= dx\n",
    "\txmax += dx\n",
    "\n",
    "\tx = np.linspace(xmin, xmax, 500)\n",
    "\tkde0_x = kde0(x)\n",
    "\tkde1_x = kde1(x)\n",
    "\n",
    "\tidx = np.argwhere(np.diff(np.sign(kde0_x - kde1_x))).flatten()\n",
    "\n",
    "\tthreshold = line_intersection([x, kde0_x], [x, kde0_x], [[x,y] for x,y in zip (x[idx], kde1_x[idx])])\n",
    "\n",
    "\treturn threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregate the congresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Budget Special Interest',\n",
       " 'Regulation General Interest',\n",
       " 'Regulation Special Interest',\n",
       " 'Domestic Social Policy',\n",
       " 'Defense Policy Budget',\n",
       " 'Defense Policy Resolutions',\n",
       " 'Foreign Policy Budget',\n",
       " 'Foreign Policy Resolutions',\n",
       " 'Budget General Interest',\n",
       " 'D. C.',\n",
       " 'Government Organization',\n",
       " 'Indian Affairs',\n",
       " 'Internal Organization']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clausen_cat_list = [n for n in G.nodes() if G.nodes[n]['bipartite'] == 0]\n",
    "peltzman_cat_list = [n for n in G.nodes() if G.nodes[n]['bipartite'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Budget Special Interest': 'Budget_Special_Interest',\n",
       " 'Regulation General Interest': 'Regulation_General_Interest',\n",
       " 'Regulation Special Interest': 'Regulation_Special_Interest',\n",
       " 'Domestic Social Policy': 'Domestic_Social_Policy',\n",
       " 'Defense Policy Budget': 'Defense_Policy_Budget',\n",
       " 'Defense Policy Resolutions': 'Defense_Policy_Resolutions',\n",
       " 'Foreign Policy Budget': 'Foreign_Policy_Budget',\n",
       " 'Foreign Policy Resolutions': 'Foreign_Policy_Resolutions',\n",
       " 'Budget General Interest': 'Budget_General_Interest',\n",
       " 'D. C.': 'D._C.',\n",
       " 'Government Organization': 'Government_Organization',\n",
       " 'Indian Affairs': 'Indian_Affairs',\n",
       " 'Internal Organization': 'Internal_Organization'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clausen_cat_dic = {x: re.sub(' ', '_', x) for x in clausen_cat_list}\n",
    "peltzman_cat_dic = {x: re.sub(' ', '_', x) for x in peltzman_cat_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../dataset/votes_house_clausen', exist_ok=True)\n",
    "os.makedirs('../dataset/votes_senate_clausen', exist_ok=True)\n",
    "\n",
    "os.makedirs()\n",
    "\n",
    "os.makedirs('../dataset/votes_house_peltzman', exist_ok=True)\n",
    "os.makedirs('../dataset/votes_senate_peltzman', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "\tos.makedirs('../dataset/votes_house_clausen/'+clausen, exist_ok=True)\n",
    "\tos.makedirs('../dataset/votes_senate_clausen/'+clausen, exist_ok=True)\n",
    "\n",
    "for peltzman in peltzman_cat_dic.values():\n",
    "\tos.makedirs('../dataset/votes_house_peltzman/'+peltzman, exist_ok=True)\n",
    "\tos.makedirs('../dataset/votes_senate_peltzman/'+peltzman, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 32/41 [02:16<00:37,  4.20s/it]/tmp/ipykernel_11888/3611659921.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_congress = pd.read_csv(csv)\n",
      "100%|██████████| 41/41 [03:19<00:00,  4.87s/it]\n"
     ]
    }
   ],
   "source": [
    "for csv in tqdm(glob.glob(congress_path)):\n",
    "    n_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\n",
    "    df_congress = pd.read_csv(csv)\n",
    "\n",
    "    for i in clausen_cat_dic.keys():\n",
    "        df_congress[df_congress['Clausen'] == i].to_csv('../dataset/votes_house_clausen/'+clausen_cat_dic[i]+'/congress_'+str(n_congress)+'.csv', index=False)\n",
    "\n",
    "    for i in peltzman_cat_dic.keys():\n",
    "        df_congress[df_congress['Peltzman'] == i].to_csv('../dataset/votes_house_peltzman/'+peltzman_cat_dic[i]+'/congress_'+str(n_congress)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in tqdm(glob.glob(congress_path_senate)):\n",
    "    n_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\n",
    "    df_congress = pd.read_csv(csv)\n",
    "\n",
    "    for i in clausen_cat_dic.keys():\n",
    "        df_congress[df_congress['Clausen'] == i].to_csv('../dataset/votes_senate_clausen/'+clausen_cat_dic[i]+'/congress_'+str(n_congress)+'.csv', index=False)\n",
    "\n",
    "    for i in peltzman_cat_dic.keys():\n",
    "        df_congress[df_congress['Peltzman'] == i].to_csv('../dataset/votes_senate_peltzman/'+peltzman_cat_dic[i]+'/congress_'+str(n_congress)+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "USA_Congress_SigNetEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
