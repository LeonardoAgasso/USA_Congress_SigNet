{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic disaggregation\n",
    "\n",
    "### Repeat the same process on data previously disaggregated according to the categories of the vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import glob, os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "from shapely.geometry import LineString\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../local/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_members_df(members, party_codes):\n",
    "    temp_congress = members.groupby('icpsr', as_index=False)[['congress']].agg(lambda x: list(x))                                                           # group by icpsr and aggregate the congress numbers into a list\n",
    "    temp_party = members.groupby('icpsr', as_index=False)[['party_code']].agg(lambda x: list(set(x)))                                                       # group by icpsr and aggregate the party codes into a list\n",
    "    temp_congress = temp_congress.merge(temp_party)                                                                                                         # merge the two dataframes\n",
    "    temp_congress['bioname'] = temp_congress['icpsr'].map(members[['icpsr', 'bioname']].set_index('icpsr').to_dict()['bioname'])                            # insert the bioname based on the icpsr \n",
    "    temp_congress['state_abbrev'] = temp_congress['icpsr'].map(members[['icpsr', 'state_abbrev']].set_index('icpsr').to_dict()['state_abbrev'])             # insert the state_abbrev based on the icpsr\n",
    "    party_codes_dic = party_codes[['party_name', 'party_code']].set_index('party_code').to_dict()['party_name']                                             # create a dictionary for the party codes\n",
    "    temp_congress['party_name'] = temp_congress['party_code'].apply(lambda x: [party_codes_dic[y] for y in x])                                              # insert the party name based on the party code\n",
    "    return temp_congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = pd.read_csv('../dataset/HSall_members.csv')\n",
    "party_codes = pd.read_csv('../dataset/HSall_parties.csv')\n",
    "\n",
    "members_info = create_members_df(members, party_codes)\n",
    "\n",
    "member_party_dict = members_info.set_index('icpsr')['party_name'].to_dict()\t\t# member_id -> party_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edgelist_from_congress(congress, members_party_dict):\n",
    "\tedgelist = pd.DataFrame()\n",
    "\n",
    "\tfor voteid in tqdm(set(congress['id'])):                 # iterate over all votes id (ids are unique for each vote)\n",
    "\n",
    "\t\ttemp = congress[congress['id'] == voteid]            # select the rows where the vote id is equal to the current vote id            \n",
    "\n",
    "\t\tyy = temp[temp['vote']=='Yea']['icpsr']              # select the icpsr of the members that voted \"Yea\"\n",
    "\t\tnn = temp[temp['vote']=='Nay']['icpsr']                         \n",
    "\n",
    "\t\ty = itertools.combinations(yy, 2)                    # all possible combinations of 2 members that voted \"Yea\"\n",
    "\t\tn = itertools.combinations(nn, 2)                \n",
    "\t\to = itertools.product(yy, nn)                        # cartesian product of the 2 series\n",
    "\n",
    "\t\ty = pd.DataFrame(y, columns = ['source', 'target'])  # create a dataframe from the combinations of \"Yea\" voters\n",
    "\t\ty['weight'] = 1                                      # add a column with the weight of the edge\n",
    "\t\ty['count'] = 1                                         \n",
    "\n",
    "\t\tn = pd.DataFrame(n, columns = ['source', 'target'])     \n",
    "\t\tn['weight'] = 1                                         \n",
    "\t\tn['count'] = 1                                          \n",
    "\n",
    "\t\to = pd.DataFrame(o, columns = ['source', 'target'])     \n",
    "\t\to['weight'] = -1                                     # same but the link is negative                    \n",
    "\t\to['count'] = 1                                          \n",
    "\n",
    "\t\tedgelist = pd.concat([edgelist, y, n, o])\n",
    "\t\tedgelist = edgelist.groupby(['source', 'target', 'weight']).sum().reset_index()     # group by source, target and weight and sum the count                  \n",
    "\n",
    "\tedgelist = pd.concat([edgelist, pd.DataFrame({\n",
    "\t\t'source': edgelist['target'],                        # new columns based on old columns: \n",
    "\t\t'target': edgelist['source'],                        #   'newcolumn': dataframe['oldcolumn']\n",
    "\t\t'weight': edgelist['weight'],\n",
    "\t\t'count': edgelist['count']})])\n",
    "\n",
    "\tedgelist = edgelist.loc[edgelist['source'] < edgelist['target']]                    # remove duplicates\n",
    "\tedgelist = edgelist.groupby(['source', 'target', 'weight']).sum().reset_index()     # group by source, target and weight and sum the count\n",
    "\tedgelist['party'] = edgelist.apply(lambda row: 'in' if members_party_dict[row['source']] == members_party_dict[row['target']] else 'out', axis=1)   # create a column with the party of the edge\n",
    "\n",
    "\tmap_votes = edgelist.groupby(['source', 'target'])['count'].sum().to_dict()                                                                         # create a dictionary with the number of votes togheter for each pair of nodes                               \n",
    "\n",
    "\tedgelist['votes_togheter'] = edgelist[['source', 'target']].apply(lambda x: map_votes[(x['source'], x['target'])], axis=1)\n",
    "\tedgelist['perc'] = edgelist['count']/edgelist['votes_togheter']\n",
    "\n",
    "\treturn edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_path = '../dataset/download_votes_merged/*'\n",
    "congress_path_senate = '../dataset/download_votes_merged_senate/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 32/41 [00:27<00:08,  1.04it/s]/tmp/ipykernel_10765/3165551236.py:7: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "100%|██████████| 41/41 [00:38<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a weighted bipartite graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for csv_file in tqdm(glob.glob(congress_path)):\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Count the occurrences of each pair of Clausen and Peltzman categories\n",
    "    counts = df.groupby([\"Clausen\", \"Peltzman\"]).size().reset_index(name=\"count\")\n",
    "    \n",
    "    # Add edges to the graph with weights based on the counts\n",
    "    for _, row in counts.iterrows():\n",
    "        clausen_category = row[\"Clausen\"]\n",
    "        peltzman_category = row[\"Peltzman\"]\n",
    "        count = row[\"count\"]\n",
    "        \n",
    "        G.add_node(clausen_category, bipartite=0)\n",
    "        G.add_node(peltzman_category, bipartite=1)\n",
    "        G.add_edge(clausen_category, peltzman_category, weight=count)\n",
    "\n",
    "# Now you have a weighted bipartite graph (G) with edges representing the co-occurrences\n",
    "# between Clausen and Peltzman categories and weights representing the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clausen categories:  ['Agriculture', 'Civil Liberties', 'Foreign and Defense Policy', 'Government Management', 'Miscellaneous Policy', 'Social Welfare']\n",
      "Number of Clausen categories:  6\n",
      "Peltzman categories:  ['Budget Special Interest', 'Regulation General Interest', 'Regulation Special Interest', 'Domestic Social Policy', 'Defense Policy Budget', 'Defense Policy Resolutions', 'Foreign Policy Budget', 'Foreign Policy Resolutions', 'Budget General Interest', 'D. C.', 'Government Organization', 'Indian Affairs', 'Internal Organization']\n",
      "Number of Peltzman categories:  13\n"
     ]
    }
   ],
   "source": [
    "print('Clausen categories: ', [n for n in G.nodes() if G.nodes[n]['bipartite'] == 0])\n",
    "print('Number of Clausen categories: ', len([n for n in G.nodes() if G.nodes[n]['bipartite'] == 0]))\n",
    "\n",
    "print('Peltzman categories: ', [n for n in G.nodes() if G.nodes[n]['bipartite'] == 1])\n",
    "print('Number of Peltzman categories: ', len([n for n in G.nodes() if G.nodes[n]['bipartite'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clausen_cat_list = [n for n in G.nodes() if G.nodes[n]['bipartite'] == 0]\n",
    "peltzman_cat_list = [n for n in G.nodes() if G.nodes[n]['bipartite'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clausen_cat_dic = {x: re.sub(' ', '_', x) for x in clausen_cat_list}\n",
    "peltzman_cat_dic = {x: re.sub(' ', '_', x) for x in peltzman_cat_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute all of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaggregate the congresses and create proper datasets\n",
    "This procedure takes time so <span style=\"color:red\">skip the next cells if you already have the disaggregated datasets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../dataset/votes_house_clausen', exist_ok=True)\n",
    "os.makedirs('../dataset/votes_senate_clausen', exist_ok=True)\n",
    "\n",
    "os.makedirs('../dataset/votes_house_peltzman', exist_ok=True)\n",
    "os.makedirs('../dataset/votes_senate_peltzman', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "\tos.makedirs('../dataset/votes_house_clausen/' + clausen, exist_ok=True)\n",
    "\tos.makedirs('../dataset/votes_senate_clausen/' + clausen, exist_ok=True)\n",
    "\n",
    "for peltzman in peltzman_cat_dic.values():\n",
    "\tos.makedirs('../dataset/votes_house_peltzman/' + peltzman, exist_ok=True)\n",
    "\tos.makedirs('../dataset/votes_senate_peltzman/' + peltzman, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 32/41 [02:00<00:34,  3.84s/it]/tmp/ipykernel_867/1185449784.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_congress = pd.read_csv(csv)\n",
      "100%|██████████| 41/41 [03:00<00:00,  4.39s/it]\n"
     ]
    }
   ],
   "source": [
    "for csv in tqdm(glob.glob(congress_path)):\n",
    "    n_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\n",
    "    df_congress = pd.read_csv(csv)\n",
    "\n",
    "    for i in clausen_cat_dic.keys():\n",
    "        filtered_df = df_congress[df_congress['Clausen'] == i]\n",
    "        if not filtered_df.empty:\n",
    "            filtered_df.to_csv('../dataset/votes_house_clausen/'+clausen_cat_dic[i]+'/congress_'+str(n_congress)+'.csv', index=False)\n",
    "\n",
    "    for i in peltzman_cat_dic.keys():\n",
    "        filtered_df = df_congress[df_congress['Peltzman'] == i]\n",
    "        if not filtered_df.empty:\n",
    "            filtered_df.to_csv('../dataset/votes_house_peltzman/'+peltzman_cat_dic[i]+'/congress_'+str(n_congress)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:35<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for csv in tqdm(glob.glob(congress_path_senate)):\n",
    "    n_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\n",
    "    df_congress = pd.read_csv(csv)\n",
    "\n",
    "    for i in clausen_cat_dic.keys():\n",
    "        filtered_df = df_congress[df_congress['Clausen'] == i]\n",
    "        if not filtered_df.empty:\n",
    "            filtered_df.to_csv('../dataset/votes_senate_clausen/'+clausen_cat_dic[i]+'/congress_'+str(n_congress)+'.csv', index=False)\n",
    "\n",
    "    for i in peltzman_cat_dic.keys():\n",
    "        filtered_df = df_congress[df_congress['Peltzman'] == i]\n",
    "        if not filtered_df.empty:\n",
    "            filtered_df.to_csv('../dataset/votes_senate_peltzman/'+peltzman_cat_dic[i]+'/congress_'+str(n_congress)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create edgelists\n",
    "This procedure takes time so <span style=\"color:red\">skip the next cells if you already have the edgelists of the disaggregated datasets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../dataset/edgelists/', exist_ok=True)\n",
    "\n",
    "os.makedirs('../dataset/edgelists/votes_house_clausen_edges/', exist_ok=True)\n",
    "os.makedirs('../dataset/edgelists/votes_house_peltzman_edges/', exist_ok=True)\n",
    "\n",
    "os.makedirs('../dataset/edgelists/votes_senate_clausen_edges/', exist_ok=True)\n",
    "os.makedirs('../dataset/edgelists/votes_senate_peltzman_edges/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "\tos.makedirs('../dataset/edgelists/votes_house_clausen_edges/'+clausen, exist_ok=True)\n",
    "\tos.makedirs('../dataset/edgelists/votes_senate_clausen_edges/'+clausen, exist_ok=True)\n",
    "\n",
    "for peltzman in peltzman_cat_dic.values():\n",
    "\tos.makedirs('../dataset/edgelists/votes_house_peltzman_edges/'+peltzman, exist_ok=True)\n",
    "\tos.makedirs('../dataset/edgelists/votes_senate_peltzman_edges/'+peltzman, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create house of representatives edgelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "\tfor csv in glob.glob('../dataset/votes_house_clausen/'+clausen+'/*'):\n",
    "\t\tn_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\t\tdf_congress = pd.read_csv(csv)\n",
    "\t\tedgelist = create_edgelist_from_congress(df_congress, member_party_dict)\n",
    "\t\tedgelist.to_csv('../dataset/edgelists/votes_house_clausen_edges/'+clausen+'/congress_'+str(n_congress)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for peltzman in peltzman_cat_dic.values():\n",
    "\tfor csv in glob.glob('../dataset/votes_house_peltzman/'+peltzman+'/*'):\n",
    "\t\tn_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\t\tdf_congress = pd.read_csv(csv)\n",
    "\t\tedgelist = create_edgelist_from_congress(df_congress, member_party_dict)\n",
    "\t\tedgelist.to_csv('../dataset/edgelists/votes_house_peltzman_edges/'+peltzman+'/congress_'+str(n_congress)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create senate edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "\tfor csv in glob.glob('../dataset/votes_senate_clausen/'+clausen+'/*'):\n",
    "\t\tn_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\t\tdf_congress = pd.read_csv(csv)\n",
    "\t\tedgelist = create_edgelist_from_congress(df_congress, member_party_dict)\n",
    "\t\tedgelist.to_csv('../dataset/edgelists/votes_senate_clausen_edges/'+clausen+'/congress_'+str(n_congress)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for peltzman in peltzman_cat_dic.values():\n",
    "\tfor csv in glob.glob('../dataset/votes_senate_peltzman/'+peltzman+'/*'):\n",
    "\t\tn_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\t\tdf_congress = pd.read_csv(csv)\n",
    "\t\tedgelist = create_edgelist_from_congress(df_congress, member_party_dict)\n",
    "\t\tedgelist.to_csv('../dataset/edgelists/votes_senate_peltzman_edges/'+peltzman+'/congress_'+str(n_congress)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the 79th house edgelist (Category \"Civil_Liberties\") since it appears to be problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:00<00:00, 10.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  9.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# create the edgelist of the 79th congress in clausen category \"Civil_Liberties\"\n",
    "df = pd.read_csv('../dataset/votes_house_clausen/Civil_Liberties/congress_79.csv')\n",
    "edgelist = create_edgelist_from_congress(df, member_party_dict)\n",
    "edgelist.to_csv('../dataset/edgelists/votes_house_clausen_edges/Civil_Liberties/congress_79.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and create thresholds files for the edgelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_singular_covariance_matrix(df):\n",
    "\trelevant_columns = ['weight', 'perc']\n",
    "\tsubset_df = df[relevant_columns]\n",
    "\n",
    "\t# Calculate the covariance matrix\n",
    "\tcovariance_matrix = np.cov(subset_df, rowvar=False)\n",
    "\n",
    "\t# Check if the determinant is zero (indicating a singular covariance matrix)\n",
    "\tis_singular = np.linalg.det(covariance_matrix) == 0\n",
    "\n",
    "\tif is_singular: return True\n",
    "\telse: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_threshold_intersx(df, weight):\t#df is the edgelist, weight is the weight of the edge\n",
    "\n",
    "\tdef _midpoint(p1, p2):\n",
    "\t\treturn {'x': (p1['x']+p2['x'])/2, 'y': (p1['y']+p2['y'])/2}\n",
    "\n",
    "\tdef line_intersection(in_party, out_party, intersect_points):\n",
    "\t\tindex_in = np.argmax(in_party[1])\n",
    "\t\tindex_out = np.argmax(out_party[1])\n",
    "\n",
    "        # points of the mean of the distributions \n",
    "\t\tpoint_in={'x': in_party[0][index_in], 'y': in_party[1][index_in]}\n",
    "\t\tpoint_out={'x': out_party[0][index_out], 'y': out_party[1][index_out]}\n",
    "\n",
    "        # medianpoint (mean of the means) of the two distributions\n",
    "\t\tmidpoint = _midpoint(point_in, point_out)\n",
    "        \n",
    "        #find index of intersection closer to midpoint\n",
    "\t\tindex_closer = np.argmin([np.sqrt( (p[0] - midpoint['x'])**2 + (p[1] - midpoint['y'])**2 ) for p in intersect_points])\n",
    "\n",
    "        # return x value of closer intersection\n",
    "\t\treturn intersect_points[index_closer][0]\n",
    "\n",
    "\tx0 = df.loc[(df['party']=='in')&(df['weight'] == weight)]['perc']\n",
    "\tx1 = df.loc[(df['party']=='out')&(df['weight'] == weight)]['perc']\n",
    "    \n",
    "\tbw = len(x0)**(-1./(2+4))\n",
    "\tkde0 = gaussian_kde(x0, bw_method=bw)\n",
    "\tbw = len(x1)**(-1./(2+4))\n",
    "\tkde1 = gaussian_kde(x1, bw_method=bw)\n",
    "\n",
    "\txmin = min(x0.min(), x1.min())\n",
    "\txmax = max(x0.max(), x1.max())\n",
    "\tdx = 0.2 * (xmax - xmin) # add a 20% margin, as the kde is wider than the data\n",
    "\txmin -= dx\n",
    "\txmax += dx\n",
    "\n",
    "\tx = np.linspace(xmin, xmax, 500)\n",
    "\tkde0_x = kde0(x)\n",
    "\tkde1_x = kde1(x)\n",
    "\tinters_x = np.minimum(kde0_x, kde1_x)\n",
    "\n",
    "\tidx = np.argwhere(np.diff(np.sign(kde0_x - kde1_x))).flatten()\n",
    "    \n",
    "\tthreshold = line_intersection([x, kde0_x], [x, kde0_x], [[x,y] for x,y in zip (x[idx], kde1_x[idx])])\n",
    "\tarea_inters_x = np.trapz(inters_x, x)\n",
    "\n",
    "\treturn threshold, area_inters_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir_name = '../dataset/thresholds/'\n",
    "\n",
    "os.makedirs(main_dir_name, exist_ok=True)\n",
    "\n",
    "os.makedirs(main_dir_name + 'house_clausen_thresholds/', exist_ok=True)\n",
    "os.makedirs(main_dir_name + 'house_peltzman_thresholds/', exist_ok=True)\n",
    "\n",
    "os.makedirs(main_dir_name + 'senate_clausen_thresholds/', exist_ok=True)\n",
    "os.makedirs(main_dir_name + 'senate_peltzman_thresholds/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "\tos.makedirs(main_dir_name + 'house_clausen_thresholds/' + clausen, exist_ok=True)\n",
    "\tos.makedirs(main_dir_name + 'senate_clausen_thresholds/' + clausen, exist_ok=True)\n",
    "\n",
    "for peltzman in peltzman_cat_dic.values():\n",
    "\tos.makedirs(main_dir_name + 'house_peltzman_thresholds/' + peltzman, exist_ok=True)\n",
    "\tos.makedirs(main_dir_name + 'senate_peltzman_thresholds/' + peltzman, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "    for csv in glob.glob('../dataset/edgelists/votes_house_clausen_edges/' + clausen + '/*'):\n",
    "        n_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "        df_congress = pd.read_csv(csv)\n",
    "\n",
    "        # Check if the covariance matrix is singular\n",
    "        is_singular = check_singular_covariance_matrix(df_congress)\n",
    "        if is_singular: continue\n",
    "\n",
    "        threshold_pos, area_pos = compute_threshold_intersx(df_congress, 1)\n",
    "        threshold_neg, area_neg = compute_threshold_intersx(df_congress, -1)\n",
    "\n",
    "        # Create a DataFrame with the desired columns and header\n",
    "        df_output = pd.DataFrame({\n",
    "            'pos_threshold': [threshold_pos],\n",
    "            'pos_area': [area_pos],\n",
    "            'neg_threshold': [threshold_neg],\n",
    "            'neg_area': [area_neg]\n",
    "        })\n",
    "\n",
    "        # Save the DataFrame to CSV with the specified filename and header\n",
    "        output_filename = f'{n_congress}_dic_thresholds_norm.csv'\n",
    "        df_output.to_csv('../dataset/thresholds/house_clausen_thresholds/' + clausen + '/' + output_filename, header=True, index=False)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the procedure on a single congress\n",
    "The execution of the following 3 cells is unnecessary, it allows to test the procedure on a single congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde(df, weight):\n",
    "    \n",
    "    def _midpoint(p1, p2):\n",
    "        return {'x': (p1['x']+p2['x'])/2, 'y': (p1['y']+p2['y'])/2}\n",
    "\n",
    "    def line_intersection(in_party, out_party, intersect_points):\n",
    "        index_in = np.argmax(in_party[1])\n",
    "        index_out = np.argmax(out_party[1])\n",
    "\n",
    "        # points of the mean of the distributions \n",
    "        point_in={'x': in_party[0][index_in], 'y': in_party[1][index_in]}\n",
    "        point_out={'x': out_party[0][index_out], 'y': out_party[1][index_out]}\n",
    "\n",
    "        # medianpoint (mean of the means) of the two distributions\n",
    "        midpoint = _midpoint(point_in, point_out)\n",
    "        \n",
    "        #find index of intersection closer to midpoint\n",
    "        index_closer = np.argmin([np.sqrt( (p[0] - midpoint['x'])**2 + (p[1] - midpoint['y'])**2 ) for p in intersect_points])\n",
    "\n",
    "        # return x value of closer intersection\n",
    "        return intersect_points[index_closer][0]\n",
    "        \n",
    "    x0 = df.loc[(df['party']=='in')&(df['weight'] == weight)]['perc']\n",
    "    x1 = df.loc[(df['party']=='out')&(df['weight'] == weight)]['perc']\n",
    "    \n",
    "    bw = len(x0)**(-1./(2+4))\n",
    "    kde0 = gaussian_kde(x0, bw_method=bw)\n",
    "    bw = len(x1)**(-1./(2+4))\n",
    "    kde1 = gaussian_kde(x1, bw_method=bw)\n",
    "\n",
    "    xmin = min(x0.min(), x1.min())\n",
    "    xmax = max(x0.max(), x1.max())\n",
    "    dx = 0.2 * (xmax - xmin) # add a 20% margin, as the kde is wider than the data\n",
    "    xmin -= dx\n",
    "    xmax += dx\n",
    "\n",
    "    x = np.linspace(xmin, xmax, 500)\n",
    "    kde0_x = kde0(x)\n",
    "    kde1_x = kde1(x)\n",
    "    inters_x = np.minimum(kde0_x, kde1_x)\n",
    "\n",
    "    idx = np.argwhere(np.diff(np.sign(kde0_x - kde1_x))).flatten()\n",
    "    \n",
    "    threshold = line_intersection([x, kde0_x], [x, kde0_x], [[x,y] for x,y in zip (x[idx], kde1_x[idx])])\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(1, 1, figsize=(17, 13))\n",
    "    fig1.set_size_inches(20, 12)\n",
    "\n",
    "    ax1.plot(x, kde0_x, color='b', label='intra-party')\n",
    "    ax1.fill_between(x, kde0_x, 0, color='b', alpha=0.2)\n",
    "\n",
    "    ax1.plot(x, kde1_x, color='orange', label='inter-party')\n",
    "    ax1.fill_between(x, kde1_x, 0, color='orange', alpha=0.2)\n",
    "\n",
    "    ax1.plot(x, inters_x, color='tomato')\n",
    "    ax1.fill_between(x, inters_x, 0, facecolor='none', edgecolor='tomato', label='intersection', alpha=0.5, hatch='xx')\n",
    "    \n",
    "    ax1.axvspan(threshold-0.0025, threshold+0.0025,color='tomato', alpha=0.7, zorder=10)\n",
    "    ax1.text(threshold-.25, .93, 'threshold: '+str(round(threshold, 2)), fontsize=23,transform=ax1.transAxes)\n",
    "    \n",
    "    area_inters_x = np.trapz(inters_x, x)\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    labels[2] += f': {area_inters_x * 100:.1f} %'\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    plt.xlabel('Edges percentage', fontsize=25)\n",
    "    plt.ylabel('Density', fontsize=25)\n",
    "    title = \"Positive edges\" if weight == 1 else \"Negative edges\"\n",
    "    c_title = \"g\" if weight == 1 else \"r\"\n",
    "    plt.title(title, fontsize=31, pad=10, ha='left', x=.0, c=c_title)\n",
    "\n",
    "    legend1 = plt.legend([handles[0],handles[1]], [labels[0],labels[1]], loc='upper center', bbox_to_anchor=(0.4, 1.08), frameon=False, ncol=2, fontsize=23)\n",
    "    plt.legend([handles[2]], [labels[2]], loc='upper center', bbox_to_anchor=(0.84, 1.08), frameon=False, ncol=1, fontsize=23)\n",
    "    plt.gca().add_artist(legend1)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    ax1.set_xlim([-0.07, 1.1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_category_set = 'clausen'\t\t# 'clausen' or 'peltzman'\n",
    "\n",
    "########################################################################################################################################################################\n",
    "# \tClausen categories: \t'Agriculture', 'Civil Liberties', 'Foreign and Defense Policy', 'Government Management', 'Miscellaneous Policy', 'Social Welfare'\t\t   #\n",
    "# \tPeltzman categories:\t'Budget Special Interest', 'Regulation General Interest', 'Regulation Special Interest', 'Domestic Social Policy', \t\t\t\t\t\t   #\n",
    "# \t\t\t\t\t\t\t'Defense Policy Budget', 'Defense Policy Resolutions', 'Foreign Policy Budget', 'Foreign Policy Resolutions', \t\t\t\t\t\t\t   #\n",
    "# \t\t\t\t\t\t\t'Budget General Interest', 'D. C.', 'Government Organization', 'Indian Affairs', 'Internal Organization'\t\t\t\t\t\t\t\t   #\n",
    "########################################################################################################################################################################\n",
    "test_category = 'Civil_Liberties'\n",
    "\n",
    "test_chamber = 'house'\t\t\t\t# 'house' or 'senate\n",
    "test_n_congress = 77\t\t\t\t# from 77 to 117\n",
    "\n",
    "test_congress = '../dataset/votes_' + test_chamber + '_' + test_category_set + '/' + test_category + '/congress_' + str(test_n_congress) + '.csv'\n",
    "test_edgelist = '../dataset/edgelists/votes_' + test_chamber + '_' + test_category_set + '_edges/' + test_category + '/congress_' + str(test_n_congress) + '.csv'\n",
    "\n",
    "df_test_congress = pd.read_csv(test_congress)\n",
    "df_test_edges = pd.read_csv(test_edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function that check the collinearity of witght and perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde(df_test_edges, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold_pos:  0.7176170522863909\n",
      "threshold_neg:  0.2879509018036072\n",
      "intersection area:  0.03750795352594417\n"
     ]
    }
   ],
   "source": [
    "threshold_pos, inters_x = compute_threshold_intersx(df_test_edges, 1)\n",
    "threshold_neg, _ = compute_threshold_intersx(df_test_edges, -1)\n",
    "print('threshold_pos: ', threshold_pos)\n",
    "print('threshold_neg: ', threshold_neg)\n",
    "print('intersection area: ', inters_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "\tfor csv in glob.glob('../dataset/edgelists/votes_house_clausen_edges/'+clausen+'/*'):\n",
    "\t\tn_congress = int(os.path.basename(csv).replace('congress_', '').replace('.csv', ''))\n",
    "\t\tdf_congress = pd.read_csv(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all the possible plots in a proper folder\n",
    "<span style=\"color:red\">There are many memory issues here... do not create plots with for-cycles as for now</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_save_kde(df, weight, main_dir_name, category_set, category, chamber, n_congress):\n",
    "\n",
    "    def _midpoint(p1, p2):\n",
    "        return {'x': (p1['x']+p2['x'])/2, 'y': (p1['y']+p2['y'])/2}\n",
    "\n",
    "    def line_intersection(in_party, out_party, intersect_points):\n",
    "        index_in = np.argmax(in_party[1])\n",
    "        index_out = np.argmax(out_party[1])\n",
    "\n",
    "        # points of the mean of the distributions \n",
    "        point_in={'x': in_party[0][index_in], 'y': in_party[1][index_in]}\n",
    "        point_out={'x': out_party[0][index_out], 'y': out_party[1][index_out]}\n",
    "\n",
    "        # medianpoint (mean of the means) of the two distributions\n",
    "        midpoint = _midpoint(point_in, point_out)\n",
    "        \n",
    "        #find index of intersection closer to midpoint\n",
    "        index_closer = np.argmin([np.sqrt( (p[0] - midpoint['x'])**2 + (p[1] - midpoint['y'])**2 ) for p in intersect_points])\n",
    "\n",
    "        # return x value of closer intersection\n",
    "        return intersect_points[index_closer][0]\n",
    "        \n",
    "    label = \"agree\" if weight == 1 else \"disagree\"\n",
    "\n",
    "    x0 = df.loc[(df['party']=='in')&(df['weight'] == weight)]['perc']\n",
    "    x1 = df.loc[(df['party']=='out')&(df['weight'] == weight)]['perc']\n",
    "    \n",
    "    bw = len(x0)**(-1./(2+4))\n",
    "    kde0 = gaussian_kde(x0, bw_method=bw)\n",
    "    bw = len(x1)**(-1./(2+4))\n",
    "    kde1 = gaussian_kde(x1, bw_method=bw)\n",
    "\n",
    "    xmin = min(x0.min(), x1.min())\n",
    "    xmax = max(x0.max(), x1.max())\n",
    "    dx = 0.2 * (xmax - xmin) # add a 20% margin, as the kde is wider than the data\n",
    "    xmin -= dx\n",
    "    xmax += dx\n",
    "\n",
    "    x = np.linspace(xmin, xmax, 500)\n",
    "    kde0_x = kde0(x)\n",
    "    kde1_x = kde1(x)\n",
    "    inters_x = np.minimum(kde0_x, kde1_x)\n",
    "\n",
    "    idx = np.argwhere(np.diff(np.sign(kde0_x - kde1_x))).flatten()\n",
    "    \n",
    "    threshold = line_intersection([x, kde0_x], [x, kde0_x], [[x,y] for x,y in zip (x[idx], kde1_x[idx])])\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(1, 1, figsize=(17, 13))\n",
    "    fig1.set_size_inches(20, 12)\n",
    "\n",
    "    ax1.plot(x, kde0_x, color='b', label='intra-party')\n",
    "    ax1.fill_between(x, kde0_x, 0, color='b', alpha=0.2)\n",
    "\n",
    "    ax1.plot(x, kde1_x, color='orange', label='inter-party')\n",
    "    ax1.fill_between(x, kde1_x, 0, color='orange', alpha=0.2)\n",
    "\n",
    "    ax1.plot(x, inters_x, color='tomato')\n",
    "    ax1.fill_between(x, inters_x, 0, facecolor='none', edgecolor='tomato', label='intersection', alpha=0.5, hatch='xx')\n",
    "    \n",
    "    ax1.axvspan(threshold-0.0025, threshold+0.0025,color='tomato', alpha=0.7, zorder=10)\n",
    "    ax1.text(threshold-.25, .93, 'threshold: '+str(round(threshold, 2)), fontsize=23,transform=ax1.transAxes)\n",
    "    \n",
    "    area_inters_x = np.trapz(inters_x, x)\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    labels[2] += f': {area_inters_x * 100:.1f} %'\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    plt.xlabel('Edges percentage', fontsize=25)\n",
    "    plt.ylabel('Density', fontsize=25)\n",
    "    title = \"Positive edges\" if weight == 1 else \"Negative edges\"\n",
    "    c_title = \"g\" if weight == 1 else \"r\"\n",
    "    plt.title(title, fontsize=31, pad=10, ha='left', x=.0, c=c_title)\n",
    "\n",
    "    legend1 = plt.legend([handles[0],handles[1]], [labels[0],labels[1]], loc='upper center', bbox_to_anchor=(0.4, 1.08), frameon=False, ncol=2, fontsize=23)\n",
    "    plt.legend([handles[2]], [labels[2]], loc='upper center', bbox_to_anchor=(0.84, 1.08), frameon=False, ncol=1, fontsize=23)\n",
    "    plt.gca().add_artist(legend1)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    plt.ioff()\n",
    "    \n",
    "    ax1.set_xlim([-0.07, 1.1])\n",
    "    \n",
    "    plt.savefig(main_dir_name + chamber + '_' + category_set + '_kdeplots/' + category + '/congress_' + str(n_congress) + '_' + label + '.png', dpi=300)\n",
    "\n",
    "    plt.close(fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir_name = '../dataset/kdeplots/'\n",
    "\n",
    "os.makedirs(main_dir_name, exist_ok=True)\n",
    "\n",
    "os.makedirs(main_dir_name + 'house_clausen_kdeplots/', exist_ok=True)\n",
    "os.makedirs(main_dir_name + 'house_peltzman_kdeplots/', exist_ok=True)\n",
    "\n",
    "os.makedirs(main_dir_name + 'senate_clausen_kdeplots/', exist_ok=True)\n",
    "os.makedirs(main_dir_name + 'senate_peltzman_kdeplots/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clausen in clausen_cat_dic.values():\n",
    "\tos.makedirs(main_dir_name + 'house_clausen_kdeplots/' + clausen, exist_ok=True)\n",
    "\tos.makedirs(main_dir_name + 'senate_clausen_kdeplots/' + clausen, exist_ok=True)\n",
    "\n",
    "for peltzman in peltzman_cat_dic.values():\n",
    "\tos.makedirs(main_dir_name + 'house_peltzman_kdeplots/' + peltzman, exist_ok=True)\n",
    "\tos.makedirs(main_dir_name + 'senate_peltzman_kdeplots/' + peltzman, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main idea was to produce GIFs with the evolution of the kde plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "USA_Congress_SigNetEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
